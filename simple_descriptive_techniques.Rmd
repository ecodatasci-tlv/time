---
title: "Simple Descriptive Techniques for Time Series Analysis"
author: "Amir R"
output:
  html_document:
    df_print: paged
---
```{r Packages, eval=T, include=FALSE}
library(magrittr) #for the pipes
# install.packages("magrittr") if not installed
```

#Objectives




***


Let's load some sample data
```{r}
load("TSsimple.RData")
```

and examine its structure

```{r}
blowfly<-read.csv("blowfly.csv", header=T) #delete later
str(blowfly)
```
***
Time for a first look at Nicholson's flies 
```{r}
plot(blowfly$total)
summary(blowfly$total)
```

and using a time series plotting method:
```{r}
plot.ts(blowfly$total)
```

#The `ts` class

`plot.ts` plots the object as if it were of the special class of R objects known simply as *ts*

the `ts()` function is used to convert objects to the ts class

```{r}
flies<-ts(blowfly$total)
class(flies)
typeof(flies)
```
many base/stats functions are actually designed for the 'ts' class
example:`acf`
```{r}
plot(flies)
acf(flies)
y.filter <- filter(flies, filter = rep(1, 5))/5
plot(y.filter)

deltat(flies)

plot(window(flies,100,200,1))

```
but using  them on a non- *ts* object may or may not result in an error:
<!-- actually all of these work.... -->
```{r eval=FALSE, include=FALSE}
acf(blowfly$total)

y.filter <- filter(blowfly$total, filter = rep(1, 5))/5
plot(y.filter)

deltat(blowfly$total)
frequency(blowfly$total)
```

Warning: the `ts` class is designed only for equidistant series. other packages have other classes and functions which extend capabilities

***
#descriptive techniques and time series exploration

let's return to our example
```{r echo=FALSE}
plot(flies)
```

This is an example of a relatively clear cyclical process, at least at the begining (points 1:200). the second part (200:361) looks less regular and with an upward trend.

##Exploratory Questions

 * what are the cycle periods ?
 * is there any hidden periodical behaviour that is not immediately apparent ?
 * Do these 2 parts behave similarly or is there a fundamental change ? which attributes  of the first part are conserved in the second part
 * can we tie this behaviour to external parameters ?

Other, not so clear examples:
```{r synthetic series, include=FALSE}
w<-rnorm(500,0,1) #this is actually a perfect white noise
rw<-cumsum(rnorm(100,0,1))
rwd<-cumsum(rnorm(200,0,1)+0.05) #random walk with drift
sine.wn<- sin(1:200/5) + rnorm(200,mean=20,sd=1.5)+seq(0,3,length.out =200)
Z<-rnorm(250,0,1)
Y<-numeric(250)
Y[1]<-Z[1]
for(i in 2:250) Y[i]<- -0.5*Y[i-1]+Z[i]

```

```{r}
par(mfrow=c(2,2))

plot.ts(w)
plot.ts(rwd)
plot.ts(sine.wn)
plot.ts(Y)

par(mfrow=c(1,1))
```

We can see some clearer cyclical behaviour of a section if we zoom in and explore a bit with `window()`:

```{r}
plot(window(flies,1,100))
plot(window(flies,1,100,deltat = 5))
plot(window(flies,1,100,deltat = 10))
plot(window(flies,1,100,deltat = 20))
```
We finally lose the cyclical behaviour at ~detat=20. let's remember this...



The flies series is unusually clear, and still - its cyclical behaviour is only one possible pattern.

other series may represent processes that behave in a way that is not immediately recognizable, and have many things going at once.
example:
```{r}

```


We naturally want  tools that will describe the entire series' and parts of the series' attributes, be exectuable fast in R, and guide us in further steps (such as model fitting and forecasting). 

for that, we need to undestand autocorrelation and related attributes of time series

##Measures of serial dependence 

###Background on Autocorrelation - The lag plot.

how does my data behave in relation to itself ? to illustrate the background for autocorrelation:

let's look at a lag of 1 point, or 'how does this week's population relate to last week's population ?

```{r}
plot(flies[1:350],flies[2:351])
```

Clearly, there's a pattern at lag =1, at least in the smaller population sizes

now let's look at other lags:

```{r fig.show='hold'}
#lag=2
plot(flies[1:350],flies[3:352])
#lag=3
plot(flies[1:350],flies[4:353])
```

we start losing the pattern gradually, as we go to higher lags...


more lags, lag = 1 to lag =8, using `lag.plot()`

```{r eval=FALSE, include=FALSE, results='hide'}
par(mfrow=c(2,4))
sapply(1:8,function (x) plot(flies[-c(1:x)],flies[-c(361:(361-x+1))]))
```
```{r}
lag.plot(flies,lag=8,layout=c(2,4),diag = T)
```

however, at lags 9-16:

```{r eval=FALSE, message=FALSE, include=FALSE, results='hide'}
par(mfrow=c(2,4))
sapply(9:16,function (x) plot( flies[-c(1:x)],flies[-c(361:(361-x+1))]))
par(mfrow=c(1,1))
```

```{r}
lag.plot(flies,lags=8,set.lags=9:16,layout=c(2,4),diag = T)
```

**summary**

 * we begin with a pattern, lose it, and then find it again
 * at smaller populations, we begin with a positive correlation, lose it, then a negative correlation  which peaks at about lag = 10, lose it, and so the cycle goes.


All of this is quite cumbersome !

### The Correlogram, using `acf()`
```{r}
acf(flies)
```

This summarizes and charts the autocorrelation structure in the series, up to a default lag size, which is 25 in this case.

The vertical axis is the relative magnitude of the  **Sample Autocorrelation Coefficients**:
let's have a look at them


```{r results='hide'}
a<-acf(flies, plot = F)
```

```{r }
a$acf %>% head(10) #for the actual factors
```

and the horizontal axis is simply the lag size.

```{r echo = FALSE}
acf(flies)
```

there's no better evidence of cycles, the flies exhibit strong positive AND negative correlation at regular intervals throughout the series, with a cycle period of 19 weeks.



let's look at the entire series
```{r}
acf(flies, lag.max = 360,col="red")
```

We can see

* the relative power of correlation at different lags
* the (sort of regular) cyclical pattern, espescially at the beginning
* the dimininshing positive correlation
* some alternation between positve and negative correlations, which is typical of cycles.

warning: there is bias here with incresing lag size. why ?

it's fair to conclude, without further evidnece to the contrary, that perhaps

  * there's some kind of a diminishing but long term "memory" in the system, up to a lag of 100 weeks. events that happened 50 weeks ago still have an influence.
  * clear cycles with a period of 19 weeks
  * there's a **LOT** of non-randomness in the sequence

This last point is espesically important for linear and other modeling, which assume that samples are independent. we may want to stop and think before going on to perform a conventional `lm()` ! , and at the very least be aware of series' behaviour.

and this illustrates why we need to explore time series intenrally before we go on to the other, external parameters.

```{r echo=FALSE, fig.show='hold'}
par(mfrow=c(2,1))

plot(flies,main=NULL)
acf(flies, lag.max = 360, main=NULL)

par(mfrow=c(1,1))
```

The **Sample Autocovariance Coeefficients**

We could also look at the correspoding covariance coefficients by change the type parameter.
```{r}
acf(flies,type="covariance")$acf %>% head(10)
```


***
###Partial Autocorrelation
Correlograms produced using the sample *autocorrelation* coefficients do not account for the fact that for a given lag size there may be correlation between internal points, (eg. values that are 4 points apart were correlated, but so do values that are 2 points apart).

We sometimes want to control for the internal correlations inside the lag, or in other words, to check what would have been the correlation coefficients, had all the internal lags' coefficients were forced to zero. 

the **Partial Autocorrelation ** is the relationship between this week's population and the population at lag n when we have already controlled for the correlations between all od the successive weeks between this week and week n

We only need to use `pacf` or `acf(type="partial")` instead of the default

```{r}
#Autocorrelation Coefficients
acc<-a$acf %>% head(12) %>% round(2)
#Partial Autocorrelation Coefficients
partial.acc<-pacf(flies, plot = F) %>% .$acf %>% head(12) %>% round(2)

plot(1:12,acc,xlab = "Lag",ylab="Coefficient")
points(1:12,partial.acc,col = "green")
lines(1:12,rep(0,12))
legend("topright", c("ac coefficients", "Partial ac coeffients"),col = c("black","green"),pch = c(1,1))

pacf(flies)
pacf(flies, lag.max = 360,col="red")
```
what can we take from this ?
the maximal negative and maximal positive lags are important places in the cycle.
clues for possible underlying processes in lags 3 and 12

the PACF can be thought of as analogous to a derivative - points driving change in pitch or direction. 


***

Summary till now, using just Correlograms for the entire series
* cycle length - about 19
* Lags 2-4, 12-16 are suspected: we should look for biological mechanisms there, when we construct an explanatory model.

again, this is only from the time series itself, before considering any other data.


###Cross Correlation
The total number of flies is not the only one we have...

we could always plotting the dataframe variables against eachother
```{r}
plot(blowfly,lower.panel = NULL)

```

we get a strong correlation between total, deaths, and emerging. but there's an inherent problem in this analysis...

```{r, fig.show='hold'}
blowfly %>% ts ->bf
plot(bf)


blowfly$eggs %>% ts ->eggs
blowfly$nonemerging %>% ts ->nonemerging
blowfly$emerging %>% ts ->emerging

#ccf(eggs,flies)

ccf(nonemerging,flies)
ccf(emerging,flies)
ccf(eggs,flies)
```
Reminder: Lags 2-4 (negative), 12-16 (positive) are suspected from `pacf()` of the total of "flies"

from `ccf()`

example, eggs vs. total- 

* negative at lags 2-3 
* negative at (-17 - -16)
* positive at 12-13

it can be difficult to construct an explanation from this alone, but we have clues of lead times and delay times.

This is a first step in time series stats, towards modeling.

Note: positive and negative lags are non-symmetric when in comes to correlation coefficients.


findings:
cycle length = still 19

larval processes *appear* to drive the cycles when they reach a maximum (density dependent competition ?)

***

###Spectral Analysis
an alternative approach to analysing fluctuations is to analyze frequencies.
this is based on Fourier transformation which give us the correlation between the time series in question and sine/cosine waves with various frequencies

####The Spectrogram
```{r}
spectrum(flies,main="")
```
in this analysis we look for frequency peaks. in this case:
```{r}
spec<-spectrum(flies)

(spec$spec) %>% which.max ->maxloc #where's the maximal value ?
max.freq<-(spec$freq)[maxloc] #what's the maximal value ?
max.freq
plot(spec, main="")
abline(v=max.freq, col="red",lwd=2)

1/max.freq
```

suggests again cycles of ~19 years.
the information may or may not be easier to interpret than a correlogram.
it's best to use both when exploring.

***
##analysing parts of a TS + detrending

now let's adapt the analysis for the two parts, separately


```{r, fig.show='hold'}
first<-ts(flies[1:200])
second<-ts(flies[201:361])
par(mfrow=c(1,2))
plot(first, ylim = c(0,14000))
plot(second)
par(mfrow=c(1,1))
```
what are the notable differences between the 2 parts ?
<!-- e.g same cycle periods ? more 'condensed' ? -->
***

1.visually identify a breakpoint

2. apply the same steps on parts of the series to find their attributes and identify discontinuities
```{r acf on both parts, fig.show="hold"}
par(mfrow=c(2,2))
acf(first)
acf(second)
pacf(first)
pacf(second)
par(mfrow=c(1,1))
```
shows generally the same attributes between the 2 parts

 * same shapes
 * cycle lengths is still 19 weeks
 * Important Lags in the *second* parts are shifted 1-2 or (-1) - (-2) lags , compared with the *first* - change points are somewhat closer to each other
 
 
*** 
however - there's a problem....
<!-- the first and second parts appear to follow different trends -->
***
### de-trending using `lm()`
let's detrend using linear regression, and have look at the results.
```{r de-trending usin a LM, results='hold'}
firstX<-1:length(first)
first.lm<-lm(first~firstX)
first.detrended<-first-predict(first.lm)

secondX<-1:length(second)
second.lm<-lm(second~secondX)
second.detrended<-second-predict(second.lm)

par(mfrow=c(2,2))
plot(first)
lines(first.lm$fitted.values, col="red")
plot(second)
lines(second.lm$fitted.values, col="blue")

plot.ts(first.detrended) #reminder Q: why plot.ts ?
plot.ts(second.detrended)
par(mfrow=c(1,1))

```
```{r}
first.lm$coefficients
second.lm$coefficients
```
note: the pitch of of 'second' is 5 times that of first


now, let's compare again
```{r acf on both parts, detrended, fig.show="hold"}
par(mfrow=c(2,2))
acf(first.detrended)
acf(second.detrended)
pacf(first.detrended)
pacf(second.detrended)
par(mfrow=c(1,1))
```
comparing the de-trended versions:
`acf()` - results similar, but `pacf()` are quite different

conclusions:

* suspected trend change
* continuity in cycle period at the same time as a trend change, 
* quite a difference in the underlying change points
* also: second - curious new negative partial coefficient at lag 18

***

### de-trending using `diff()`

first order differencing will also coerce the series to stationarity, and will do it more severely while still keeping the attributes of the signal

```{r}
par(mfrow=c(2,1))
plot(flies)
flies %>% diff %>% plot
par(mfrow=c(1,1))
```
it has several uses and the advantage that parameters do not need to be estimated (e.g. from a linear model)



```{r}
first.diff<-diff(first)
second.diff<-diff(second)


par(mfrow=c(3,2))
plot(first)
lines(first.lm$fitted.values, col="red")
plot(second)
lines(second.lm$fitted.values, col="blue")

plot.ts(first.detrended) #reminder Q: why plot.ts ?
plot.ts(second.detrended)

plot(first.diff)
plot(second.diff)

par(mfrow=c(1,1))
```

```{r}
par(mfrow=c(3,2))

plot(first)
plot(second)

acf(first.detrended, main="")
acf(second.detrended,main="")

acf(first.diff, main="")
acf(second.diff, main="")
par(mfrow=c(1,1))
```
Differencing is more successfull in creating
***

##Smoothing
let's return to some noisier series

```{r}
w<-ts(w)
rwd<-ts(rwd)
sine.wn<-ts(sine.wn)
Y<-ts(Y)

plot(w)
plot(rwd)
plot(sine.wn)
plot(Y)
```
last time we used `window()`
```{r}
par(mfrow=c(2,2))

plot(window(w, 100, 200))
plot(window(w, 100, 150))
plot(window(w, 100, 125))

par(mfrow=c(1,1))
```
this approach doesn't work

we should probably start looking by smoothing first

###smoothing via `filter()`

filter allows many methods for smoothing

#### filtering with a moving average

this is the simplest technique
we use the filter argument with a weights vector, giving equal weights
```{r, fig.show="hold"}
par(mfrow = c(3, 1))

v <- filter(w, sides = 2, filter = rep(1, 3) / 3)
plot(w)
lines(v, col = "red")

rwd.s <- filter(rwd, sides = 2, filter = rep(1, 3) / 3)
plot(rwd)
lines(rwd.s, col = "red")

sine.wn.s <- filter(sine.wn, sides = 2, filter = rep(1, 3) / 3)
plot(sine.wn)
lines(sine.wn.s, col = "red")

Y.s <- filter(Y, sides = 2, filter = rep(1, 3) / 3)
plot(Y)
lines(Y.s, col = "red")

par(mfrow = c(1, 1))
```


let's try a 9 point average
```{r,fig.show="hold"}
v <- filter(w, sides = 2, filter = rep(1, 9) / 9)
plot(w)
lines(v, col = "red")

rwd.s <- filter(rwd, sides = 2, filter = rep(1, 9) / 9)
plot(rwd)
lines(rwd.s, col = "red")

sine.wn.s <- filter(sine.wn, sides = 2, filter = rep(1, 9) / 9)
plot(sine.wn)
lines(sine.wn.s, col = "red")


Y.s <- filter(Y, sides = 2, filter = rep(1, 9) / 9)
plot(Y)
lines(Y.s, col = "red")
```

let's try a 15 point average
```{r,fig.show="hold"}
v<-filter(w,sides=2, filter = rep(1,15)/15)
plot(w)
lines(v, col = "red")

rwd.s <- filter(rwd, sides = 2, filter = rep(1, 15) / 15)
plot(rwd)
lines(rwd.s, col = "red")

sine.wn.s<-filter(sine.wn,sides=2, filter = rep(1,15)/15)
plot(sine.wn)
lines(sine.wn.s, col = "red", lwd=3)


Y.s <- filter(Y, sides = 2, filter = rep(1, 15) / 15)
plot(Y)
lines(Y.s, col = "red")
```

increase the avergaing period to recieve a smoother results and (maybe) reveal patterns in the noise


a moving average is a *local smoother*, meaning it allows a change of series behavior throught the series.

###smoothing via `ksmooth()`
```{r}
t <- 1:length(sine.wn)

plot(sine.wn)
lines(sine.wn.s, col = "red",lwd=3)
lines(ksmooth(t, sine.wn, "normal", bandwidth = 5), col="blue",lwd=2)

plot(sine.wn)
lines(sine.wn.s, col = "red",lwd=3)
lines(ksmooth(t, sine.wn, "normal", bandwidth = 10), col="blue",lwd=2)

plot(sine.wn)
lines(sine.wn.s, col = "red",lwd=3)
lines(ksmooth(t, sine.wn, "normal", bandwidth = 20), col="blue",lwd=2)
```

The wider the bandwidth, the smoother the result

###smoothing via `smooth.spline()`
```{r}
t <- 1:length(sine.wn)

#smoothing paraeter=0.1
plot(sine.wn)
lines(sine.wn.s, col = "red",lwd=3)
lines(smooth.spline(t, sine.wn, spar=0.1), col="blue",lwd=2)

#smoothing paraeter=1
plot(sine.wn)
lines(sine.wn.s, col = "red",lwd=3)
lines(smooth.spline(t, sine.wn, spar=1), col="blue",lwd=2)

#smoothing paraeter=0.7
plot(sine.wn)
lines(sine.wn.s, col = "red",lwd=3)
lines(smooth.spline(t, sine.wn, spar=0.7), col="blue",lwd=2)

```
###smoothing via several methods
```{r}
t <- 1:length(flies)

plot(flies)
lines(ksmooth(t, flies, "normal", bandwidth = 5), col="blue",lwd=2)

#spline smoothing smoothing paraeter=0.4
plot(flies)
lines(smooth.spline(t, flies, spar=0.4), col="blue",lwd=2)

```

***


## typical series and interpretations
```{r}
par(mfrow=c(3,1))
plot(w)
acf(w,main="")
spectrum(w,main="")
par(mfrow=c(1,1))
```
No definitive pattern in lags beyond 0 or a dominant frequecy

typical of a *Pure White Noise* process, which is Normal errors around a steady mean. 

```{r}
par(mfrow=c(3,1))
plot(ts(rw))
acf(rw, main="")
spectrum(rw,main="")
par(mfrow=c(1,1))
```
No definitive pattern in lags, but high correlation for all lags

typical of a *Random Walk* process, where values are correlated to (mostly) only the latest value (the values in the previous step), plus a small *white noise* 

**Super Important **
Is there a real trend here ? the time plot and our brain say there is. but the underlying process is probably not a *real* trend or combination of trends, but 'drift'.

the next value is more likely to be a minor deviation from the last value than a result of a another external driver ! 
random walks are notorious in deceiving our brains.
actually, the long term mean of the formula used to create this series,
`rw<-cumsum(rnorm(100,0,1))`
**is 0 !**

repeat:

```{r}
rw<-cumsum(rnorm(100,0,1))
par(mfrow=c(3,1))
plot(ts(rw))
acf(rw, main="")
spectrum(rw,main="")
par(mfrow=c(1,1))
```
The same ACF evnthough the timeplot is completely different

```{r}
par(mfrow=c(3,1))
plot(Y)
acf(Y)
pacf(Y,main="")
par(mfrow=c(1,1))
```
repeated negative followed by positive:
return to equilibrium following random departures from it.

This is called an **Autoregressive Process** of order 1 (the order can be determined from the `pacf()`), or 'AR(1)' 
in this case a *negative* AR(1) with rapid respone

ecological models of equilibrium population dynamics are typically Autoregressive.

## Conclusion & Remarks


# Source Material and useful links


